import numpy as np # type: ignore
import matplotlib.pyplot as plt

# Load the true value table to calculate MSE score
true_value=np.load("value_table.npy")

#Store MSE score each iteration
MSE=[]
Y=[]
class GridworldDP_2:
    def __init__(self, grid_size=(50, 50), targets=[(0, 0), (49, 49)], gamma=0.9):
        self.grid_size = grid_size
        self.targets = targets
        self.gamma = gamma  # Discount factor
        self.values = np.zeros(grid_size)  # Initialize value function for each state
        self.policy = np.full(grid_size, None)  # Policy: what action to take in each state
        self.actions = ['up', 'down', 'left', 'right']  # Available actions
        self.dem=0
    def get_possible_actions(self, state):
        """Returns the list of valid actions in the current state (to avoid moving out of the grid)."""
        row, col = state
        possible_actions = []
        if row > 0: possible_actions.append('up')
        if row < self.grid_size[0] - 1: possible_actions.append('down')
        if col > 0: possible_actions.append('left')
        if col < self.grid_size[1] - 1: possible_actions.append('right')
        return possible_actions

    def step(self, state, action):
        """Returns the next state and reward given the current state and action."""
        row, col = state
        if action == 'up':
            next_state = (row - 1, col)
        elif action == 'down':
            next_state = (row + 1, col)
        elif action == 'left':
            next_state = (row, col - 1)
        elif action == 'right':
            next_state = (row, col + 1)
        else:
            next_state = state

        # Check if we reached a target
        if next_state in self.targets:
            reward = -1  # Reaching target
        else:
            reward = -1  # Every step costs -1

        return next_state, reward

    def policy_evaluation(self, theta=0.0001):
        """Evaluate the current policy by calculating the value function."""
        
        sum=0
        while True:
            delta = 0
            self.dem+=1
            
            for row in range(self.grid_size[0]):
                for col in range(self.grid_size[1]):
                    state = (row, col)
                    
                    # Skip target states
                    if state in self.targets:
                        continue

                    v = self.values[state]
                    action = self.policy[state]
                
                    # Compute value for the current policy
                    next_state, reward = self.step(state, action)
                    self.values[state] = reward + self.gamma * self.values[next_state]
                    delta = max(delta, abs(v - self.values[state]))
                    
                    
            
            for row in range(self.grid_size[0]):
                    for col in range(self.grid_size[1]):
                        sum+=(self.values[(row,col)]-true_value[row][col])**2        
            print(sum)
            sum=sum/2500
            MSE.append(sum)    
            Y.append(self.dem)
            if delta < theta:
                break 
            
        

    def policy_improvement(self):
        """Improve the current policy by acting greedily with respect to the value function."""
        policy_stable = True
        for row in range(self.grid_size[0]):
            for col in range(self.grid_size[1]):
                state = (row, col)

                # Skip target states
                if state in self.targets:
                    self.policy[state] = 'target'
                    continue

                old_action = self.policy[state]
                action_values = []

                # Evaluate all possible actions
                for action in self.get_possible_actions(state):
                    next_state, reward = self.step(state, action)
                    action_value = reward + self.gamma * self.values[next_state]
                    action_values.append(action_value)

                # Get the action with the highest value
                best_action = self.get_possible_actions(state)[np.argmax(action_values)]
                self.policy[state] = best_action

                if old_action != best_action:
                    policy_stable = False

        return policy_stable
    def policy_iteration(self):
        
        while(True):
            
            self.policy_evaluation()
            policy_stable=self.policy_improvement()
           
            if policy_stable:
                break
    def render_values(self):
        """Display the value function."""
        print("Value Function:")
        for row in range(self.grid_size[0]):
            print([round(self.values[row, col], 2) for col in range(self.grid_size[1])])
        print()

    def render_policy(self):
        """Display the policy."""
        print("Policy:")
        for row in range(self.grid_size[0]):
            print([self.policy[(row, col)] for col in range(self.grid_size[1])])
        print()
    def sdebugger(self):
        dumsum=0
        for row in range(self.grid_size[0]):
            for col in range(self.grid_size[1]):
                dumsum+=(true_value[row][col]+1)**2
                
        print(dumsum)
                


# Example usage
env = GridworldDP_2()
env.policy_iteration()  # Run policy iteration
env.sdebugger()

                    
MSE=np.array(MSE)
Y=np.array(Y)
np.save('my_array.npy', MSE)
np.save('my_another_array.npy',Y)

        
